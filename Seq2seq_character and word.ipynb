{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Language Translation model  English to French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Character level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.python.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the inputs and encode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len= 10000\n",
    "df = 'fra.txt'\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "ip_chars = set()\n",
    "op_chars = set()\n",
    "with open(df, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "#Inputs have two parts, the English text ie input_text and input target ie target_text    \n",
    "for line in lines[: min(sample_len, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')[:2]\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in ip_chars:\n",
    "            ip_chars.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in op_chars:\n",
    "            op_chars.add(char)\n",
    "\n",
    "ip_chars = sorted(list(ip_chars))\n",
    "op_chars = sorted(list(op_chars))\n",
    "num_encoder_tokens = len(ip_chars)\n",
    "num_decoder_tokens = len(op_chars)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_tokens = dict([(char, i) for i, char in enumerate(ip_chars)])\n",
    "op_tokens = dict([(char, i) for i, char in enumerate(op_chars)])\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, ip_tokens[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, op_tokens[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, op_tokens[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the character-level model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "epochs = 50\n",
    "latent_dim = 256 \n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_45\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           [(None, None, 71)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           [(None, None, 93)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  [(None, 256), (None, 335872      input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  [(None, None, 256),  358400      input_32[0][0]                   \n",
      "                                                                 lstm_16[0][1]                    \n",
      "                                                                 lstm_16[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 93)     23901       lstm_17[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 718,173\n",
      "Trainable params: 718,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "55/55 [==============================] - 21s 376ms/step - loss: 1.0207 - val_loss: 1.0168\n",
      "Epoch 2/50\n",
      "55/55 [==============================] - 18s 331ms/step - loss: 0.8766 - val_loss: 0.9621\n",
      "Epoch 3/50\n",
      "55/55 [==============================] - 17s 309ms/step - loss: 0.8074 - val_loss: 0.8674\n",
      "Epoch 4/50\n",
      "55/55 [==============================] - 17s 308ms/step - loss: 0.7123 - val_loss: 0.7757\n",
      "Epoch 5/50\n",
      "55/55 [==============================] - 17s 310ms/step - loss: 0.6428 - val_loss: 0.7207\n",
      "Epoch 6/50\n",
      "55/55 [==============================] - 17s 311ms/step - loss: 0.5971 - val_loss: 0.6822\n",
      "Epoch 7/50\n",
      "55/55 [==============================] - 17s 313ms/step - loss: 0.5649 - val_loss: 0.6507\n",
      "Epoch 8/50\n",
      "55/55 [==============================] - 17s 312ms/step - loss: 0.5409 - val_loss: 0.6356\n",
      "Epoch 9/50\n",
      "55/55 [==============================] - 17s 311ms/step - loss: 0.5229 - val_loss: 0.6125\n",
      "Epoch 10/50\n",
      "55/55 [==============================] - 18s 329ms/step - loss: 0.5062 - val_loss: 0.5979\n",
      "Epoch 11/50\n",
      "55/55 [==============================] - 20s 368ms/step - loss: 0.4918 - val_loss: 0.5859\n",
      "Epoch 12/50\n",
      "55/55 [==============================] - 20s 360ms/step - loss: 0.4783 - val_loss: 0.5741\n",
      "Epoch 13/50\n",
      "55/55 [==============================] - 17s 312ms/step - loss: 0.4659 - val_loss: 0.5600\n",
      "Epoch 14/50\n",
      "55/55 [==============================] - 19s 348ms/step - loss: 0.4548 - val_loss: 0.5530\n",
      "Epoch 15/50\n",
      "55/55 [==============================] - 17s 316ms/step - loss: 0.4435 - val_loss: 0.5436\n",
      "Epoch 16/50\n",
      "55/55 [==============================] - 19s 353ms/step - loss: 0.4341 - val_loss: 0.5341\n",
      "Epoch 17/50\n",
      "55/55 [==============================] - 22s 409ms/step - loss: 0.4251 - val_loss: 0.5307\n",
      "Epoch 18/50\n",
      "55/55 [==============================] - 19s 348ms/step - loss: 0.4159 - val_loss: 0.5200\n",
      "Epoch 19/50\n",
      "55/55 [==============================] - 19s 338ms/step - loss: 0.4065 - val_loss: 0.5147\n",
      "Epoch 20/50\n",
      "55/55 [==============================] - 19s 344ms/step - loss: 0.3978 - val_loss: 0.5091\n",
      "Epoch 21/50\n",
      "55/55 [==============================] - 19s 354ms/step - loss: 0.3907 - val_loss: 0.5060\n",
      "Epoch 22/50\n",
      "55/55 [==============================] - 20s 371ms/step - loss: 0.3838 - val_loss: 0.5010\n",
      "Epoch 23/50\n",
      "55/55 [==============================] - 20s 362ms/step - loss: 0.3770 - val_loss: 0.4974\n",
      "Epoch 24/50\n",
      "55/55 [==============================] - 18s 328ms/step - loss: 0.3696 - val_loss: 0.4925\n",
      "Epoch 25/50\n",
      "55/55 [==============================] - 18s 329ms/step - loss: 0.3635 - val_loss: 0.4855\n",
      "Epoch 26/50\n",
      "55/55 [==============================] - 20s 355ms/step - loss: 0.3557 - val_loss: 0.4852\n",
      "Epoch 27/50\n",
      "55/55 [==============================] - 19s 347ms/step - loss: 0.3494 - val_loss: 0.4791\n",
      "Epoch 28/50\n",
      "55/55 [==============================] - 19s 343ms/step - loss: 0.3425 - val_loss: 0.4774\n",
      "Epoch 29/50\n",
      "55/55 [==============================] - 18s 333ms/step - loss: 0.3366 - val_loss: 0.4762\n",
      "Epoch 30/50\n",
      "55/55 [==============================] - 21s 381ms/step - loss: 0.3305 - val_loss: 0.4722\n",
      "Epoch 31/50\n",
      "55/55 [==============================] - 21s 385ms/step - loss: 0.3249 - val_loss: 0.4696\n",
      "Epoch 32/50\n",
      "55/55 [==============================] - 21s 385ms/step - loss: 0.3194 - val_loss: 0.4690\n",
      "Epoch 33/50\n",
      "55/55 [==============================] - 19s 354ms/step - loss: 0.3140 - val_loss: 0.4650\n",
      "Epoch 34/50\n",
      "55/55 [==============================] - 19s 352ms/step - loss: 0.3087 - val_loss: 0.4627\n",
      "Epoch 35/50\n",
      "55/55 [==============================] - 20s 356ms/step - loss: 0.3023 - val_loss: 0.4612\n",
      "Epoch 36/50\n",
      "55/55 [==============================] - 20s 356ms/step - loss: 0.2969 - val_loss: 0.4610\n",
      "Epoch 37/50\n",
      "55/55 [==============================] - 18s 332ms/step - loss: 0.2917 - val_loss: 0.4618\n",
      "Epoch 38/50\n",
      "55/55 [==============================] - 19s 340ms/step - loss: 0.2873 - val_loss: 0.4580\n",
      "Epoch 39/50\n",
      "55/55 [==============================] - 18s 334ms/step - loss: 0.2815 - val_loss: 0.4588\n",
      "Epoch 40/50\n",
      "55/55 [==============================] - 18s 325ms/step - loss: 0.2766 - val_loss: 0.4595\n",
      "Epoch 41/50\n",
      "55/55 [==============================] - 21s 389ms/step - loss: 0.2720 - val_loss: 0.4576\n",
      "Epoch 42/50\n",
      "55/55 [==============================] - 18s 328ms/step - loss: 0.2667 - val_loss: 0.4584\n",
      "Epoch 43/50\n",
      "55/55 [==============================] - 19s 351ms/step - loss: 0.2626 - val_loss: 0.4561\n",
      "Epoch 44/50\n",
      "55/55 [==============================] - 22s 395ms/step - loss: 0.2580 - val_loss: 0.4582\n",
      "Epoch 45/50\n",
      "55/55 [==============================] - 21s 379ms/step - loss: 0.2539 - val_loss: 0.4594\n",
      "Epoch 46/50\n",
      "55/55 [==============================] - 18s 336ms/step - loss: 0.2487 - val_loss: 0.4595\n",
      "Epoch 47/50\n",
      "55/55 [==============================] - 22s 392ms/step - loss: 0.2438 - val_loss: 0.4585\n",
      "Epoch 48/50\n",
      "55/55 [==============================] - 22s 392ms/step - loss: 0.2409 - val_loss: 0.4636\n",
      "Epoch 49/50\n",
      "55/55 [==============================] - 21s 375ms/step - loss: 0.2364 - val_loss: 0.4608\n",
      "Epoch 50/50\n",
      "55/55 [==============================] - 17s 311ms/step - loss: 0.2322 - val_loss: 0.4645\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.3)\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in ip_tokens.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in op_tokens.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, op_tokens['\\t']] = 1.\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Number: 0\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Va !\n",
      "\n",
      "Sentence Number: 1\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "Sentence Number: 2\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "Sentence Number: 3\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "Sentence Number: 4\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Courez !\n",
      "\n",
      "Sentence Number: 5\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui a fait ?\n",
      "\n",
      "Sentence Number: 6\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Commence s'ast-mour.\n",
      "\n",
      "Sentence Number: 7\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Vu veu !\n",
      "\n",
      "Sentence Number: 8\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Salue !\n",
      "\n",
      "Sentence Number: 9\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Attrapez-moi.\n",
      "\n",
      "Sentence Number: 10\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "Sentence Number: 11\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "Sentence Number: 12\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "Sentence Number: 13\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "Sentence Number: 14\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "Sentence Number: 15\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "Sentence Number: 16\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "Sentence Number: 17\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Montez.\n",
      "\n",
      "Sentence Number: 18\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Sais !\n",
      "\n",
      "Sentence Number: 19\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Sais !\n",
      "\n",
      "Sentence Number: 20\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Je l'ai suis.\n",
      "\n",
      "Sentence Number: 21\n",
      "Input sentence: I try.\n",
      "Decoded sentence: J'ai été préparé.\n",
      "\n",
      "Sentence Number: 22\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai été préparé.\n",
      "\n",
      "Sentence Number: 23\n",
      "Input sentence: I won!\n",
      "Decoded sentence: J'ai été préparé.\n",
      "\n",
      "Sentence Number: 24\n",
      "Input sentence: I won.\n",
      "Decoded sentence: Je vous ai sauvée.\n",
      "\n",
      "Sentence Number: 25\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Pardit !\n",
      "\n",
      "Sentence Number: 26\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "Sentence Number: 27\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Attendez !\n",
      "\n",
      "Sentence Number: 28\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Laissez tont blesses.\n",
      "\n",
      "Sentence Number: 29\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Laissez tont blesses.\n",
      "\n",
      "Sentence Number: 30\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Laissez tont blesses.\n",
      "\n",
      "Sentence Number: 31\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Laissez tont blesses.\n",
      "\n",
      "Sentence Number: 32\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Allez-moi.\n",
      "\n",
      "Sentence Number: 33\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Marchez-le.\n",
      "\n",
      "Sentence Number: 34\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Marchez-le.\n",
      "\n",
      "Sentence Number: 35\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Marchez-le.\n",
      "\n",
      "Sentence Number: 36\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: C'applesez-vous.\n",
      "\n",
      "Sentence Number: 37\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: C'applesez-vous.\n",
      "\n",
      "Sentence Number: 38\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Avare-toi ?\n",
      "\n",
      "Sentence Number: 39\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Avare-toi ?\n",
      "\n",
      "Sentence Number: 40\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Avare-toi ?\n",
      "\n",
      "Sentence Number: 41\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Monte.\n",
      "\n",
      "Sentence Number: 42\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: Monte.\n",
      "\n",
      "Sentence Number: 43\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serre-moi d'an res res !\n",
      "\n",
      "Sentence Number: 44\n",
      "Input sentence: Hug me.\n",
      "Decoded sentence: Serre-moi d'an res res !\n",
      "\n",
      "Sentence Number: 45\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis sentie de bougur.\n",
      "\n",
      "Sentence Number: 46\n",
      "Input sentence: I fell.\n",
      "Decoded sentence: Je me suis sentie de bougur.\n",
      "\n",
      "Sentence Number: 47\n",
      "Input sentence: I fled.\n",
      "Decoded sentence: Je me suis partie.\n",
      "\n",
      "Sentence Number: 48\n",
      "Input sentence: I know.\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "Sentence Number: 49\n",
      "Input sentence: I left.\n",
      "Decoded sentence: J'ai été préparé.\n",
      "\n"
     ]
    }
   ],
   "source": [
    " for seq_index in range(50):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    number = str(seq_index)\n",
    "    print('Sentence Number: '+ number)\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:\n",
    "1. The character level model predicts almost all single words corectly. For example: Hi. -> Salut!. It even adds the exclamation mark!\n",
    "2. The model translated smaller sentences with little ambiguity. For example: \n",
    " * I fled. -> Je me suis partie. (which means \"I left\" from Google translate)\n",
    " * Go now. -> Marchez-le. (which means \"walk it\" from Google translate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word level model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "line= pd.read_table('fra.txt', nrows=10000, usecols=[0,1], names=['eng', 'fra'])\n",
    "\n",
    "line.eng=line.eng.apply(lambda x: x.lower())\n",
    "line.fra=line.fra.apply(lambda x: x.lower())\n",
    "\n",
    "line.eng=line.eng.apply(lambda x: re.sub(\"'\", '', str(x)))\n",
    "line.fra=line.fra.apply(lambda x: re.sub(\"'\", '', str(x)))\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "\n",
    "# Remove all the special characters\n",
    "line.eng=line.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "line.fra=line.fra.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "line.fra = line.fra.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n",
    "remove_digits = str.maketrans('', '', string.digits)\n",
    "line.eng=line.eng.apply(lambda x: x.translate(remove_digits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4568"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary of English\n",
    "all_eng_words=set()\n",
    "for eng in line.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "            \n",
    "all_german_words=set() # will have only unique words of frail\n",
    "for fra in line.fra:\n",
    "    for word in fra.split():\n",
    "        if word not in all_german_words:\n",
    "            all_german_words.add(word)\n",
    "\n",
    "# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in line.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "    \n",
    "max_length_src = np.max(lenght_list)\n",
    "max_length_src\n",
    "\n",
    "#Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in line.fra:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "    \n",
    "max_length_tar = np.max(lenght_list)\n",
    "max_length_tar\n",
    "\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_german_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_german_words)\n",
    "num_encoder_tokens, num_decoder_tokens\n",
    "\n",
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>go</td>\n",
       "      <td>START_ va  _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hi</td>\n",
       "      <td>START_ salut  _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>hi</td>\n",
       "      <td>START_ salut _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>run</td>\n",
       "      <td>START_ cours  _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>run</td>\n",
       "      <td>START_ courez  _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng                  fra\n",
       "0   go      START_ va  _END\n",
       "1   hi   START_ salut  _END\n",
       "2   hi    START_ salut _END\n",
       "3  run   START_ cours  _END\n",
       "4  run  START_ courez  _END"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "line.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000,), (2000,))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test data\n",
    "X, Y = line.eng, line.fra\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Zero arrays\n",
    "def generate_batch(X = X_train, Y = Y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], Y[j:j+batch_size])):  \n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "\n",
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the word level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens+1, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens+1, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_51\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 50)     107900      input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 50)     228450      input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  [(None, 50), (None,  20200       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  [(None, None, 50), ( 20200       embedding_5[0][0]                \n",
      "                                                                 lstm_18[0][1]                    \n",
      "                                                                 lstm_18[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, None, 4568)   232968      lstm_19[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 609,718\n",
      "Trainable params: 609,718\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Compile the Model \n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "62/62 [==============================] - 18s 283ms/step - loss: 2.3763 - acc: 0.2315 - val_loss: 1.9408 - val_acc: 0.2362\n",
      "Epoch 2/50\n",
      "62/62 [==============================] - 17s 274ms/step - loss: 1.8648 - acc: 0.2376 - val_loss: 1.9266 - val_acc: 0.2362\n",
      "Epoch 3/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.8432 - acc: 0.2373 - val_loss: 1.9366 - val_acc: 0.2362\n",
      "Epoch 4/50\n",
      "62/62 [==============================] - 17s 272ms/step - loss: 1.8040 - acc: 0.2510 - val_loss: 1.8977 - val_acc: 0.2788\n",
      "Epoch 5/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.7262 - acc: 0.2848 - val_loss: 1.8013 - val_acc: 0.2808\n",
      "Epoch 6/50\n",
      "62/62 [==============================] - 17s 272ms/step - loss: 1.6364 - acc: 0.2865 - val_loss: 1.7357 - val_acc: 0.2819\n",
      "Epoch 7/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.5748 - acc: 0.3053 - val_loss: 1.6880 - val_acc: 0.3076\n",
      "Epoch 8/50\n",
      "62/62 [==============================] - 17s 274ms/step - loss: 1.5243 - acc: 0.3156 - val_loss: 1.6537 - val_acc: 0.3154\n",
      "Epoch 9/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.4816 - acc: 0.3255 - val_loss: 1.6190 - val_acc: 0.3245\n",
      "Epoch 10/50\n",
      "62/62 [==============================] - 17s 274ms/step - loss: 1.4422 - acc: 0.3352 - val_loss: 1.5905 - val_acc: 0.3358\n",
      "Epoch 11/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.4085 - acc: 0.3463 - val_loss: 1.5677 - val_acc: 0.3406\n",
      "Epoch 12/50\n",
      "62/62 [==============================] - 17s 272ms/step - loss: 1.3761 - acc: 0.3565 - val_loss: 1.5412 - val_acc: 0.3495\n",
      "Epoch 13/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.3471 - acc: 0.3671 - val_loss: 1.5203 - val_acc: 0.3637\n",
      "Epoch 14/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.3180 - acc: 0.3822 - val_loss: 1.4996 - val_acc: 0.3780\n",
      "Epoch 15/50\n",
      "62/62 [==============================] - 17s 272ms/step - loss: 1.2902 - acc: 0.3971 - val_loss: 1.4804 - val_acc: 0.3899\n",
      "Epoch 16/50\n",
      "62/62 [==============================] - 17s 274ms/step - loss: 1.2634 - acc: 0.4072 - val_loss: 1.4600 - val_acc: 0.3979\n",
      "Epoch 17/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.2355 - acc: 0.4174 - val_loss: 1.4413 - val_acc: 0.4061\n",
      "Epoch 18/50\n",
      "62/62 [==============================] - 17s 271ms/step - loss: 1.2089 - acc: 0.4310 - val_loss: 1.4226 - val_acc: 0.4230\n",
      "Epoch 19/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.1823 - acc: 0.4430 - val_loss: 1.4035 - val_acc: 0.4292\n",
      "Epoch 20/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.1600 - acc: 0.4499 - val_loss: 1.3887 - val_acc: 0.4346\n",
      "Epoch 21/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.1342 - acc: 0.4571 - val_loss: 1.3737 - val_acc: 0.4374\n",
      "Epoch 22/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.1131 - acc: 0.4634 - val_loss: 1.3607 - val_acc: 0.4417\n",
      "Epoch 23/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.0915 - acc: 0.4699 - val_loss: 1.3487 - val_acc: 0.4479\n",
      "Epoch 24/50\n",
      "62/62 [==============================] - 17s 272ms/step - loss: 1.0716 - acc: 0.4767 - val_loss: 1.3374 - val_acc: 0.4539\n",
      "Epoch 25/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.0523 - acc: 0.4834 - val_loss: 1.3277 - val_acc: 0.4582\n",
      "Epoch 26/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.0343 - acc: 0.4910 - val_loss: 1.3190 - val_acc: 0.4595\n",
      "Epoch 27/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 1.0157 - acc: 0.4986 - val_loss: 1.3098 - val_acc: 0.4644\n",
      "Epoch 28/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 0.9997 - acc: 0.5039 - val_loss: 1.3022 - val_acc: 0.4665\n",
      "Epoch 29/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 0.9827 - acc: 0.5102 - val_loss: 1.2973 - val_acc: 0.4663\n",
      "Epoch 30/50\n",
      "62/62 [==============================] - 17s 271ms/step - loss: 0.9663 - acc: 0.5147 - val_loss: 1.2859 - val_acc: 0.4736\n",
      "Epoch 31/50\n",
      "62/62 [==============================] - 18s 284ms/step - loss: 0.9524 - acc: 0.5184 - val_loss: 1.2805 - val_acc: 0.4736\n",
      "Epoch 32/50\n",
      "62/62 [==============================] - 17s 281ms/step - loss: 0.9381 - acc: 0.5220 - val_loss: 1.2760 - val_acc: 0.4729\n",
      "Epoch 33/50\n",
      "62/62 [==============================] - 17s 280ms/step - loss: 0.9237 - acc: 0.5262 - val_loss: 1.2652 - val_acc: 0.4790\n",
      "Epoch 34/50\n",
      "62/62 [==============================] - 17s 281ms/step - loss: 0.9091 - acc: 0.5314 - val_loss: 1.2615 - val_acc: 0.4784\n",
      "Epoch 35/50\n",
      "62/62 [==============================] - 18s 293ms/step - loss: 0.8970 - acc: 0.5353 - val_loss: 1.2549 - val_acc: 0.4816\n",
      "Epoch 36/50\n",
      "62/62 [==============================] - 18s 294ms/step - loss: 0.8846 - acc: 0.5389 - val_loss: 1.2522 - val_acc: 0.4805\n",
      "Epoch 37/50\n",
      "62/62 [==============================] - 18s 283ms/step - loss: 0.8719 - acc: 0.5433 - val_loss: 1.2455 - val_acc: 0.4842\n",
      "Epoch 38/50\n",
      "62/62 [==============================] - 18s 285ms/step - loss: 0.8592 - acc: 0.5468 - val_loss: 1.2456 - val_acc: 0.4844\n",
      "Epoch 39/50\n",
      "62/62 [==============================] - 17s 272ms/step - loss: 0.8476 - acc: 0.5518 - val_loss: 1.2404 - val_acc: 0.4854\n",
      "Epoch 40/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 0.8363 - acc: 0.5553 - val_loss: 1.2397 - val_acc: 0.4847\n",
      "Epoch 41/50\n",
      "62/62 [==============================] - 17s 272ms/step - loss: 0.8244 - acc: 0.5582 - val_loss: 1.2338 - val_acc: 0.4906\n",
      "Epoch 42/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 0.8135 - acc: 0.5629 - val_loss: 1.2319 - val_acc: 0.4913\n",
      "Epoch 43/50\n",
      "62/62 [==============================] - 17s 273ms/step - loss: 0.8017 - acc: 0.5677 - val_loss: 1.2268 - val_acc: 0.4933\n",
      "Epoch 44/50\n",
      "62/62 [==============================] - 18s 287ms/step - loss: 0.7908 - acc: 0.5710 - val_loss: 1.2261 - val_acc: 0.4905\n",
      "Epoch 45/50\n",
      "62/62 [==============================] - 18s 291ms/step - loss: 0.7800 - acc: 0.5753 - val_loss: 1.2239 - val_acc: 0.4932\n",
      "Epoch 46/50\n",
      "62/62 [==============================] - 18s 289ms/step - loss: 0.7693 - acc: 0.5785 - val_loss: 1.2196 - val_acc: 0.4962\n",
      "Epoch 47/50\n",
      "62/62 [==============================] - 18s 291ms/step - loss: 0.7597 - acc: 0.5813 - val_loss: 1.2171 - val_acc: 0.4965\n",
      "Epoch 48/50\n",
      "62/62 [==============================] - 18s 283ms/step - loss: 0.7506 - acc: 0.5841 - val_loss: 1.2157 - val_acc: 0.4977\n",
      "Epoch 49/50\n",
      "62/62 [==============================] - 17s 272ms/step - loss: 0.7413 - acc: 0.5881 - val_loss: 1.2162 - val_acc: 0.4987\n",
      "Epoch 50/50\n",
      "62/62 [==============================] - 17s 271ms/step - loss: 0.7313 - acc: 0.5906 - val_loss: 1.2165 - val_acc: 0.5002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x146887a10>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the Model\n",
    "model.fit_generator(generator = generate_batch(X_train, Y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, Y_test, batch_size =batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of word level model is 59%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"Context vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_state_input)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_input,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, Y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Source sentence: are you tom\n",
      "Actual Target Translation:  êtesvous tom  \n",
      "Predicted Target Translation:  vous êtes perdu \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input Source sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Target Translation:', Y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Target Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: i want to walk\n",
      "Actual French Translation:  je veux marcher \n",
      "Predicted French Translation:  je veux des travail \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual French Translation:', Y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted French Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: grab my hand\n",
      "Actual French Translation:  saisissezmoi la main  \n",
      "Predicted French Translation:  faisle tranquille \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual French Translation:', Y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted French Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference:\n",
    "1. The word level model is not able to predict the noun in some cases. For example: In \"Are you tom\", \"are you\" is predicted correctly. But not Tom.\n",
    "2. The verb is not predicted correctly in the second sentence. \"I want to walk\"is predicted as \"je veux des travail\"(I want work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
